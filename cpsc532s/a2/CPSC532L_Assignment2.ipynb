{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you must download the dataset provided as a separate link on the course webpage and extract it into `data/`. The dataset contains approximately 20K training images and 100 validation images, with multiple captions/tags for each image. For this assignment, we are only concerned with the tags and ignore the captions.\n",
    "\n",
    "For question two on the assignment, the dataset also contains a JSON file that maps from the ImageNet labels to the category names. \n",
    "\n",
    "Following the data downloading and unzipping, the code below loads in the data into memory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0039, 0.0078, 0.0039,  ..., 0.0471, 0.0471, 0.0314],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0353, 0.0353, 0.0392],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0392, 0.0392, 0.0510],\n",
       "          ...,\n",
       "          [0.7137, 0.7294, 0.7137,  ..., 0.1686, 0.1843, 0.1686],\n",
       "          [0.7059, 0.6902, 0.6863,  ..., 0.1765, 0.1804, 0.2039],\n",
       "          [0.6784, 0.6667, 0.6706,  ..., 0.1922, 0.2157, 0.2275]],\n",
       "\n",
       "         [[0.1490, 0.1490, 0.1412,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.1451, 0.1412, 0.1373,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.1412, 0.1373, 0.1373,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.4392, 0.4667, 0.4549,  ..., 0.2588, 0.2745, 0.2863],\n",
       "          [0.4353, 0.4235, 0.4196,  ..., 0.2745, 0.2980, 0.3137],\n",
       "          [0.4118, 0.4000, 0.4000,  ..., 0.3020, 0.3176, 0.3020]],\n",
       "\n",
       "         [[0.5294, 0.5294, 0.5294,  ..., 0.1451, 0.1412, 0.1333],\n",
       "          [0.5255, 0.5333, 0.5373,  ..., 0.1725, 0.1451, 0.1412],\n",
       "          [0.5373, 0.5490, 0.5451,  ..., 0.2314, 0.1843, 0.1608],\n",
       "          ...,\n",
       "          [0.0118, 0.0078, 0.0078,  ..., 0.5216, 0.5294, 0.5137],\n",
       "          [0.0078, 0.0078, 0.0118,  ..., 0.5098, 0.5216, 0.5216],\n",
       "          [0.0078, 0.0118, 0.0039,  ..., 0.5294, 0.5255, 0.4784]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Resize(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "]) \n",
    "def load_image(filename):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor).unsqueeze(0)\n",
    "    return image_var.cuda()\n",
    "\n",
    "load_image('data/train2014/COCO_train2014_000000000009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ImageNet label to category name mapping.\n",
    "imagenet_categories = [value for key,value in sorted(json.load(open('data/imagenet_categories.json')).items(), key=lambda t: int(t[0]))]\n",
    "\n",
    "# Load annotations file for the 20K training images.\n",
    "mscoco_train = json.load(open('data/annotations/train2014.json'))\n",
    "train_ids = [entry['id'] for entry in mscoco_train['images']]\n",
    "train_id_to_file = {entry['id']: 'data/train2014/' + entry['file_name'] for entry in mscoco_train['images']}\n",
    "category_to_name = {entry['id']: entry['name'] for entry in mscoco_train['categories']}\n",
    "category_idx_to_name = [entry['name'] for entry in mscoco_train['categories']]\n",
    "category_to_idx = {entry['id']: i for i,entry in enumerate(mscoco_train['categories'])}\n",
    "\n",
    "# Load annotations file for the 100 validation images.\n",
    "mscoco_val = json.load(open('data/annotations/val2014.json'))\n",
    "val_ids = [entry['id'] for entry in mscoco_val['images']]\n",
    "val_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_val['images']}\n",
    "\n",
    "# We extract out all of the category labels for the images in the training set. We use a set to ignore \n",
    "# duplicate labels.\n",
    "train_id_to_categories = defaultdict(set)\n",
    "for entry in mscoco_train['annotations']:\n",
    "    train_id_to_categories[entry['image_id']].add(entry['category_id'])\n",
    "\n",
    "# We extract out all of the category labels for the images in the validation set. We use a set to ignore \n",
    "# duplicate labels.\n",
    "val_id_to_categories = defaultdict(set)\n",
    "for entry in mscoco_val['annotations']:\n",
    "    val_id_to_categories[entry['image_id']].add(entry['category_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at an image and its corresponding category labels. We consider the image with the id 391895 and the corresponding filename, `data/val2014/COCO_val2014_000000391895.jpg`. The image is shown below.\n",
    "\n",
    "![image](data/val2014/COCO_val2014_000000391895.jpg)\n",
    "\n",
    "The following code determines the category labels for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. person\n",
      "1. bicycle\n",
      "2. motorcycle\n"
     ]
    }
   ],
   "source": [
    "for i,category in enumerate(val_id_to_categories[391895]):\n",
    "    print(\"%d. %s\" % (i, category_to_name[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading a Pre-trained Convolutional Neural Network (CNN)\n",
    "\n",
    "We will work with the VGG-16 image classification CNN network first introduced in [Very Deep Convolutional Neural Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf) by K. Simonyan and A. Zisserman.\n",
    "\n",
    "Fairly straightforwardly, we load the pre-trained VGG model and indicate to PyTorch that we are using the model for inference rather than training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model = models.vgg16(pretrained=True).cuda()\n",
    "vgg_model.eval()\n",
    "\n",
    "# Let's see what the model looks like.\n",
    "vgg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making Predictions Using VGG-16\n",
    "\n",
    "Given the pre-trained network, we must now write the code to make predictions on the 10 validation images via a forward pass through the network. Typically the final layer of VGG-16 is a softmax layer, however the pre-trained PyTorch model that we are using does not have softmax built into the final layer (instead opting to incorporate it into the loss function) and therefore we must **manually** apply softmax to the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True value for id: 391895\n",
      "0. person\n",
      "1. bicycle\n",
      "2. motorcycle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for id: 391895\n",
      "0.15975737571716309 assault rifle, assault gun\n",
      "0.11209803819656372 stretcher\n",
      "0.08013104647397995 rifle\n",
      "0.05695946887135506 military uniform\n",
      "0.04560713842511177 jeep, landrover\n",
      "\n",
      "True value for id: 522418\n",
      "0. sink\n",
      "1. person\n",
      "2. cake\n",
      "3. knife\n",
      "\n",
      "Predictions for id: 522418\n",
      "0.2567004859447479 mosquito net\n",
      "0.08372808247804642 toilet tissue, toilet paper, bathroom tissue\n",
      "0.08117162436246872 wardrobe, closet, press\n",
      "0.04646625369787216 washbasin, handbasin, washbowl, lavabo, wash-hand basin\n",
      "0.0376296229660511 medicine chest, medicine cabinet\n",
      "\n",
      "True value for id: 184613\n",
      "0. person\n",
      "1. umbrella\n",
      "2. cow\n",
      "\n",
      "Predictions for id: 184613\n",
      "0.1697528064250946 Arabian camel, dromedary, Camelus dromedarius\n",
      "0.08139925450086594 sandbar, sand bar\n",
      "0.07311606407165527 Italian greyhound\n",
      "0.06620091944932938 Weimaraner\n",
      "0.03160366043448448 swimming trunks, bathing trunks\n",
      "\n",
      "True value for id: 318219\n",
      "0. tv\n",
      "1. person\n",
      "2. mouse\n",
      "3. keyboard\n",
      "\n",
      "Predictions for id: 318219\n",
      "0.1280694156885147 plastic bag\n",
      "0.09797225892543793 lab coat, laboratory coat\n",
      "0.06251281499862671 toilet tissue, toilet paper, bathroom tissue\n",
      "0.059056464582681656 bonnet, poke bonnet\n",
      "0.057441603392362595 gasmask, respirator, gas helmet\n",
      "\n",
      "True value for id: 554625\n",
      "0. tv\n",
      "1. person\n",
      "2. mouse\n",
      "3. keyboard\n",
      "\n",
      "Predictions for id: 554625\n",
      "0.07708378881216049 plastic bag\n",
      "0.07356971502304077 groom, bridegroom\n",
      "0.07237771898508072 gasmask, respirator, gas helmet\n",
      "0.06662335991859436 toilet tissue, toilet paper, bathroom tissue\n",
      "0.05071510002017021 mosquito net\n",
      "\n",
      "True value for id: 397133\n",
      "0. person\n",
      "1. dining table\n",
      "2. bottle\n",
      "3. oven\n",
      "4. cup\n",
      "5. knife\n",
      "6. spoon\n",
      "7. bowl\n",
      "8. sink\n",
      "9. broccoli\n",
      "10. carrot\n",
      "\n",
      "Predictions for id: 397133\n",
      "0.3610766530036926 dining table, board\n",
      "0.19368426501750946 grand piano, grand\n",
      "0.10433466732501984 mosquito net\n",
      "0.033410023897886276 restaurant, eating house, eating place, eatery\n",
      "0.019991032779216766 desk\n",
      "\n",
      "True value for id: 574769\n",
      "0. potted plant\n",
      "1. person\n",
      "2. bottle\n",
      "3. oven\n",
      "4. cat\n",
      "5. refrigerator\n",
      "6. spoon\n",
      "7. bowl\n",
      "8. clock\n",
      "9. sink\n",
      "10. orange\n",
      "11. handbag\n",
      "\n",
      "Predictions for id: 574769\n",
      "0.08988551795482635 bath towel\n",
      "0.08958843350410461 dishwasher, dish washer, dishwashing machine\n",
      "0.05552069842815399 shower cap\n",
      "0.04780123382806778 bassinet\n",
      "0.046681541949510574 hand blower, blow dryer, blow drier, hair dryer, hair drier\n",
      "\n",
      "True value for id: 60623\n",
      "0. person\n",
      "1. dining table\n",
      "2. wine glass\n",
      "3. spoon\n",
      "4. bowl\n",
      "\n",
      "Predictions for id: 60623\n",
      "0.2045636773109436 Petri dish\n",
      "0.07023518532514572 bubble\n",
      "0.038521986454725266 groom, bridegroom\n",
      "0.035935867577791214 tub, vat\n",
      "0.0335400253534317 candle, taper, wax light\n",
      "\n",
      "True value for id: 309022\n",
      "0. sink\n",
      "1. bowl\n",
      "2. bottle\n",
      "3. oven\n",
      "\n",
      "Predictions for id: 309022\n",
      "0.27247336506843567 photocopier\n",
      "0.19897709786891937 refrigerator, icebox\n",
      "0.08541078120470047 medicine chest, medicine cabinet\n",
      "0.0789535790681839 microwave, microwave oven\n",
      "0.06786021590232849 dishwasher, dish washer, dishwashing machine\n",
      "\n",
      "True value for id: 5802\n",
      "0. person\n",
      "1. bottle\n",
      "2. cup\n",
      "3. knife\n",
      "4. bowl\n",
      "5. backpack\n",
      "\n",
      "Predictions for id: 5802\n",
      "0.48053592443466187 lab coat, laboratory coat\n",
      "0.20499716699123383 mosquito net\n",
      "0.027210207656025887 hoopskirt, crinoline\n",
      "0.024719005450606346 pajama, pyjama, pj's, jammies\n",
      "0.023093335330486298 beaker\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax()\n",
    "for image_id in val_ids[:10]:\n",
    "    # Display the image.\n",
    "    # -- Your code goes here --\n",
    "    img_str = val_id_to_file[image_id]\n",
    "    pil_im = Image.open(img_str, 'r')\n",
    "    pil_im.show()\n",
    "    \n",
    "    # Print all of the category labels for this image.\n",
    "    # -- Your code goes here --\n",
    "    print(\"\\nTrue value for id: %d\" % image_id)\n",
    "    for i,category in enumerate(val_id_to_categories[image_id]):\n",
    "        print(\"%d. %s\" % (i, category_to_name[category]))\n",
    "  \n",
    "    # Load/preprocess the image.\n",
    "    img = load_image(img_str)\n",
    "\n",
    "    # Run the image through the model and softmax.\n",
    "    label_likelihoods = softmax(vgg_model(img)).squeeze()\n",
    "\n",
    "    # Get the top 5 labels, and their corresponding likelihoods.\n",
    "    probs, indices = label_likelihoods.topk(5)\n",
    "\n",
    "    # Iterate and print out the predictions.\n",
    "    # -- Your code goes here --\n",
    "    print(\"\\nPredictions for id: %d\" % image_id)\n",
    "    img_ids = [index.item() for index in indices]\n",
    "    prob_vals = [prob.item() for prob in probs]\n",
    "    for prob, id in zip(prob_vals, img_ids):\n",
    "        y_pred = imagenet_categories[id]\n",
    "        print(prob, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing Generic Visual Features using CNN\n",
    "\n",
    "Since, rather than the output of VGG, we want a fixed sized vector representation of each image, we remove the last linear layer. The implementation of the forward function for VGG is shown below:\n",
    "\n",
    "```\n",
    "x = self.features(x)\n",
    "x = x.view(x.size(0), -1)\n",
    "x = self.classifier(x)\n",
    "```\n",
    "We aim to preserve everything but the final component of the classifier, meaning we must define an alternative equivalent to `self.classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Dropout(p=0.5)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace)\n",
      "  (5): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Remove the final layer of the classifier, and indicate to PyTorch that the model is being used for inference\n",
    "# rather than training (most importantly, this disables dropout).\n",
    "\n",
    "# -- Your code goes here --\n",
    "short_classifier = vgg_model.classifier[:-1]\n",
    "print(short_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------- Vectorizing Training Images ------------------------------------\n",
      "\n",
      " 0.00% complete!\n",
      "\n",
      " 0.50% complete!\n",
      "\n",
      " 1.00% complete!\n",
      "\n",
      " 1.50% complete!\n",
      "\n",
      " 2.00% complete!\n",
      "\n",
      " 2.50% complete!\n",
      "\n",
      " 3.00% complete!\n",
      "\n",
      " 3.50% complete!\n",
      "\n",
      " 4.00% complete!\n",
      "\n",
      " 4.50% complete!\n",
      "\n",
      " 5.00% complete!\n",
      "\n",
      " 5.50% complete!\n",
      "\n",
      " 6.00% complete!\n",
      "\n",
      " 6.50% complete!\n",
      "\n",
      " 7.00% complete!\n",
      "\n",
      " 7.50% complete!\n",
      "\n",
      " 8.00% complete!\n",
      "\n",
      " 8.50% complete!\n",
      "\n",
      " 9.00% complete!\n",
      "\n",
      " 9.50% complete!\n",
      "\n",
      " 10.00% complete!\n",
      "\n",
      " 10.50% complete!\n",
      "\n",
      " 11.00% complete!\n",
      "\n",
      " 11.50% complete!\n",
      "\n",
      " 12.00% complete!\n",
      "\n",
      " 12.50% complete!\n",
      "\n",
      " 13.00% complete!\n",
      "\n",
      " 13.50% complete!\n",
      "\n",
      " 14.00% complete!\n",
      "\n",
      " 14.50% complete!\n",
      "\n",
      " 15.00% complete!\n",
      "\n",
      " 15.50% complete!\n",
      "\n",
      " 16.00% complete!\n",
      "\n",
      " 16.50% complete!\n",
      "\n",
      " 17.00% complete!\n",
      "\n",
      " 17.50% complete!\n",
      "\n",
      " 18.00% complete!\n",
      "\n",
      " 18.50% complete!\n",
      "\n",
      " 19.00% complete!\n",
      "\n",
      " 19.50% complete!\n",
      "\n",
      " 20.00% complete!\n",
      "\n",
      " 20.50% complete!\n",
      "\n",
      " 21.00% complete!\n",
      "\n",
      " 21.50% complete!\n",
      "\n",
      " 22.00% complete!\n",
      "\n",
      " 22.50% complete!\n",
      "\n",
      " 23.00% complete!\n",
      "\n",
      " 23.50% complete!\n",
      "\n",
      " 24.00% complete!\n",
      "\n",
      " 24.50% complete!\n",
      "\n",
      " 25.00% complete!\n",
      "\n",
      " 25.50% complete!\n",
      "\n",
      " 26.00% complete!\n",
      "\n",
      " 26.50% complete!\n",
      "\n",
      " 27.00% complete!\n",
      "\n",
      " 27.50% complete!\n",
      "\n",
      " 28.00% complete!\n",
      "\n",
      " 28.50% complete!\n",
      "\n",
      " 29.00% complete!\n",
      "\n",
      " 29.50% complete!\n",
      "\n",
      " 30.00% complete!\n",
      "\n",
      " 30.50% complete!\n",
      "\n",
      " 31.00% complete!\n",
      "\n",
      " 31.50% complete!\n",
      "\n",
      " 32.00% complete!\n",
      "\n",
      " 32.50% complete!\n",
      "\n",
      " 33.00% complete!\n",
      "\n",
      " 33.50% complete!\n",
      "\n",
      " 34.00% complete!\n",
      "\n",
      " 34.50% complete!\n",
      "\n",
      " 35.00% complete!\n",
      "\n",
      " 35.50% complete!\n",
      "\n",
      " 36.00% complete!\n",
      "\n",
      " 36.50% complete!\n",
      "\n",
      " 37.00% complete!\n",
      "\n",
      " 37.50% complete!\n",
      "\n",
      " 38.00% complete!\n",
      "\n",
      " 38.50% complete!\n",
      "\n",
      " 39.00% complete!\n",
      "\n",
      " 39.50% complete!\n",
      "\n",
      " 40.00% complete!\n",
      "\n",
      " 40.50% complete!\n",
      "\n",
      " 41.00% complete!\n",
      "\n",
      " 41.50% complete!\n",
      "\n",
      " 42.00% complete!\n",
      "\n",
      " 42.50% complete!\n",
      "\n",
      " 43.00% complete!\n",
      "\n",
      " 43.50% complete!\n",
      "\n",
      " 44.00% complete!\n",
      "\n",
      " 44.50% complete!\n",
      "\n",
      " 45.00% complete!\n",
      "\n",
      " 45.50% complete!\n",
      "\n",
      " 46.00% complete!\n",
      "\n",
      " 46.50% complete!\n",
      "\n",
      " 47.00% complete!\n",
      "\n",
      " 47.50% complete!\n",
      "\n",
      " 48.00% complete!\n",
      "\n",
      " 48.50% complete!\n",
      "\n",
      " 49.00% complete!\n",
      "\n",
      " 49.50% complete!\n",
      "\n",
      " 50.00% complete!\n",
      "\n",
      " 50.50% complete!\n",
      "\n",
      " 51.00% complete!\n",
      "\n",
      " 51.50% complete!\n",
      "\n",
      " 52.00% complete!\n",
      "\n",
      " 52.50% complete!\n",
      "\n",
      " 53.00% complete!\n",
      "\n",
      " 53.50% complete!\n",
      "\n",
      " 54.00% complete!\n",
      "\n",
      " 54.50% complete!\n",
      "\n",
      " 55.00% complete!\n",
      "\n",
      " 55.50% complete!\n",
      "\n",
      " 56.00% complete!\n",
      "\n",
      " 56.50% complete!\n",
      "\n",
      " 57.00% complete!\n",
      "\n",
      " 57.50% complete!\n",
      "\n",
      " 58.00% complete!\n",
      "\n",
      " 58.50% complete!\n",
      "\n",
      " 59.00% complete!\n",
      "\n",
      " 59.50% complete!\n",
      "\n",
      " 60.00% complete!\n",
      "\n",
      " 60.50% complete!\n",
      "\n",
      " 61.00% complete!\n",
      "\n",
      " 61.50% complete!\n",
      "\n",
      " 62.00% complete!\n",
      "\n",
      " 62.50% complete!\n",
      "\n",
      " 63.00% complete!\n",
      "\n",
      " 63.50% complete!\n",
      "\n",
      " 64.00% complete!\n",
      "\n",
      " 64.50% complete!\n",
      "\n",
      " 65.00% complete!\n",
      "\n",
      " 65.50% complete!\n",
      "\n",
      " 66.00% complete!\n",
      "\n",
      " 66.50% complete!\n",
      "\n",
      " 67.00% complete!\n",
      "\n",
      " 67.50% complete!\n",
      "\n",
      " 68.00% complete!\n",
      "\n",
      " 68.50% complete!\n",
      "\n",
      " 69.00% complete!\n",
      "\n",
      " 69.50% complete!\n",
      "\n",
      " 70.00% complete!\n",
      "\n",
      " 70.50% complete!\n",
      "\n",
      " 71.00% complete!\n",
      "\n",
      " 71.50% complete!\n",
      "\n",
      " 72.00% complete!\n",
      "\n",
      " 72.50% complete!\n",
      "\n",
      " 73.00% complete!\n",
      "\n",
      " 73.50% complete!\n",
      "\n",
      " 74.00% complete!\n",
      "\n",
      " 74.50% complete!\n",
      "\n",
      " 75.00% complete!\n",
      "\n",
      " 75.50% complete!\n",
      "\n",
      " 76.00% complete!\n",
      "\n",
      " 76.50% complete!\n",
      "\n",
      " 77.00% complete!\n",
      "\n",
      " 77.50% complete!\n",
      "\n",
      " 78.00% complete!\n",
      "\n",
      " 78.50% complete!\n",
      "\n",
      " 79.00% complete!\n",
      "\n",
      " 79.50% complete!\n",
      "\n",
      " 80.00% complete!\n",
      "\n",
      " 80.50% complete!\n",
      "\n",
      " 81.00% complete!\n",
      "\n",
      " 81.50% complete!\n",
      "\n",
      " 82.00% complete!\n",
      "\n",
      " 82.50% complete!\n",
      "\n",
      " 83.00% complete!\n",
      "\n",
      " 83.50% complete!\n",
      "\n",
      " 84.00% complete!\n",
      "\n",
      " 84.50% complete!\n",
      "\n",
      " 85.00% complete!\n",
      "\n",
      " 85.50% complete!\n",
      "\n",
      " 86.00% complete!\n",
      "\n",
      " 86.50% complete!\n",
      "\n",
      " 87.00% complete!\n",
      "\n",
      " 87.50% complete!\n",
      "\n",
      " 88.00% complete!\n",
      "\n",
      " 88.50% complete!\n",
      "\n",
      " 89.00% complete!\n",
      "\n",
      " 89.50% complete!\n",
      "\n",
      " 90.00% complete!\n",
      "\n",
      " 90.50% complete!\n",
      "\n",
      " 91.00% complete!\n",
      "\n",
      " 91.50% complete!\n",
      "\n",
      " 92.00% complete!\n",
      "\n",
      " 92.50% complete!\n",
      "\n",
      " 93.00% complete!\n",
      "\n",
      " 93.50% complete!\n",
      "\n",
      " 94.00% complete!\n",
      "\n",
      " 94.50% complete!\n",
      "\n",
      " 95.00% complete!\n",
      "\n",
      " 95.50% complete!\n",
      "\n",
      " 96.00% complete!\n",
      "\n",
      " 96.50% complete!\n",
      "\n",
      " 97.00% complete!\n",
      "\n",
      " 97.50% complete!\n",
      "\n",
      " 98.00% complete!\n",
      "\n",
      " 98.50% complete!\n",
      "\n",
      " 99.00% complete!\n",
      "\n",
      " 99.50% complete!\n"
     ]
    }
   ],
   "source": [
    "# First we vectorize all of the features of training images and write the results to a file.\n",
    "\n",
    "# -- Your code goes here --\n",
    "n = len(train_ids)\n",
    "training_vectors = np.empty([n, 4096])\n",
    "print(\"\\n--------------------- Vectorizing Training Images ------------------------------------\")\n",
    "for i, image_id in enumerate(train_ids):\n",
    "    # find path\n",
    "    img_str = train_id_to_file[image_id]\n",
    "\n",
    "    # load into tensor\n",
    "    image_vector = load_image(img_str)\n",
    "\n",
    "    # compute forward pass\n",
    "    x = vgg_model.features(image_vector)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = short_classifier(x)\n",
    "    training_vectors[i] = x.detach().cpu().numpy()\n",
    "    if i % 100 == 0:\n",
    "        percent_complete = (i / n) * 100\n",
    "        print(\"\\n %.2f%% complete!\" % percent_complete)\n",
    "    \n",
    "np.save(open('outputs/training_vectors', 'wb+'), training_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------- Vectorizing Validation Images ------------------------------------\n",
      "\n",
      " 0.00% complete!\n",
      "\n",
      " 1.00% complete!\n",
      "\n",
      " 2.00% complete!\n",
      "\n",
      " 3.00% complete!\n",
      "\n",
      " 4.00% complete!\n",
      "\n",
      " 5.00% complete!\n",
      "\n",
      " 6.00% complete!\n",
      "\n",
      " 7.00% complete!\n",
      "\n",
      " 8.00% complete!\n",
      "\n",
      " 9.00% complete!\n",
      "\n",
      " 10.00% complete!\n",
      "\n",
      " 11.00% complete!\n",
      "\n",
      " 12.00% complete!\n",
      "\n",
      " 13.00% complete!\n",
      "\n",
      " 14.00% complete!\n",
      "\n",
      " 15.00% complete!\n",
      "\n",
      " 16.00% complete!\n",
      "\n",
      " 17.00% complete!\n",
      "\n",
      " 18.00% complete!\n",
      "\n",
      " 19.00% complete!\n",
      "\n",
      " 20.00% complete!\n",
      "\n",
      " 21.00% complete!\n",
      "\n",
      " 22.00% complete!\n",
      "\n",
      " 23.00% complete!\n",
      "\n",
      " 24.00% complete!\n",
      "\n",
      " 25.00% complete!\n",
      "\n",
      " 26.00% complete!\n",
      "\n",
      " 27.00% complete!\n",
      "\n",
      " 28.00% complete!\n",
      "\n",
      " 29.00% complete!\n",
      "\n",
      " 30.00% complete!\n",
      "\n",
      " 31.00% complete!\n",
      "\n",
      " 32.00% complete!\n",
      "\n",
      " 33.00% complete!\n",
      "\n",
      " 34.00% complete!\n",
      "\n",
      " 35.00% complete!\n",
      "\n",
      " 36.00% complete!\n",
      "\n",
      " 37.00% complete!\n",
      "\n",
      " 38.00% complete!\n",
      "\n",
      " 39.00% complete!\n",
      "\n",
      " 40.00% complete!\n",
      "\n",
      " 41.00% complete!\n",
      "\n",
      " 42.00% complete!\n",
      "\n",
      " 43.00% complete!\n",
      "\n",
      " 44.00% complete!\n",
      "\n",
      " 45.00% complete!\n",
      "\n",
      " 46.00% complete!\n",
      "\n",
      " 47.00% complete!\n",
      "\n",
      " 48.00% complete!\n",
      "\n",
      " 49.00% complete!\n",
      "\n",
      " 50.00% complete!\n",
      "\n",
      " 51.00% complete!\n",
      "\n",
      " 52.00% complete!\n",
      "\n",
      " 53.00% complete!\n",
      "\n",
      " 54.00% complete!\n",
      "\n",
      " 55.00% complete!\n",
      "\n",
      " 56.00% complete!\n",
      "\n",
      " 57.00% complete!\n",
      "\n",
      " 58.00% complete!\n",
      "\n",
      " 59.00% complete!\n",
      "\n",
      " 60.00% complete!\n",
      "\n",
      " 61.00% complete!\n",
      "\n",
      " 62.00% complete!\n",
      "\n",
      " 63.00% complete!\n",
      "\n",
      " 64.00% complete!\n",
      "\n",
      " 65.00% complete!\n",
      "\n",
      " 66.00% complete!\n",
      "\n",
      " 67.00% complete!\n",
      "\n",
      " 68.00% complete!\n",
      "\n",
      " 69.00% complete!\n",
      "\n",
      " 70.00% complete!\n",
      "\n",
      " 71.00% complete!\n",
      "\n",
      " 72.00% complete!\n",
      "\n",
      " 73.00% complete!\n",
      "\n",
      " 74.00% complete!\n",
      "\n",
      " 75.00% complete!\n",
      "\n",
      " 76.00% complete!\n",
      "\n",
      " 77.00% complete!\n",
      "\n",
      " 78.00% complete!\n",
      "\n",
      " 79.00% complete!\n",
      "\n",
      " 80.00% complete!\n",
      "\n",
      " 81.00% complete!\n",
      "\n",
      " 82.00% complete!\n",
      "\n",
      " 83.00% complete!\n",
      "\n",
      " 84.00% complete!\n",
      "\n",
      " 85.00% complete!\n",
      "\n",
      " 86.00% complete!\n",
      "\n",
      " 87.00% complete!\n",
      "\n",
      " 88.00% complete!\n",
      "\n",
      " 89.00% complete!\n",
      "\n",
      " 90.00% complete!\n",
      "\n",
      " 91.00% complete!\n",
      "\n",
      " 92.00% complete!\n",
      "\n",
      " 93.00% complete!\n",
      "\n",
      " 94.00% complete!\n",
      "\n",
      " 95.00% complete!\n",
      "\n",
      " 96.00% complete!\n",
      "\n",
      " 97.00% complete!\n",
      "\n",
      " 98.00% complete!\n",
      "\n",
      " 99.00% complete!\n"
     ]
    }
   ],
   "source": [
    "# Next we vectorize all of the features of validation images and write the results to a file.\n",
    "    \n",
    "# -- Your code goes here --\n",
    "n = len(val_ids)\n",
    "validation_vectors = np.empty([n, 4096])\n",
    "print(\"\\n--------------------- Vectorizing Validation Images ------------------------------------\")\n",
    "for i, image_id in enumerate(val_ids):\n",
    "    # find path\n",
    "    img_str = val_id_to_file[image_id]\n",
    "\n",
    "    # load into tensor\n",
    "    image_vector = load_image(img_str)\n",
    "\n",
    "    # compute forward pass\n",
    "    x = vgg_model.features(image_vector)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = short_classifier(x)\n",
    "    validation_vectors[i] = x.detach().cpu().numpy()\n",
    "    percent_complete = (i / n) * 100\n",
    "    print(\"\\n %.2f%% complete!\" % percent_complete)\n",
    "\n",
    "np.save(open('outputs/validation_vectors', 'wb+'), validation_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visual Similarity\n",
    "\n",
    "We now use the generated vectors, to find the closest training image for each validation image. This can easily be done by finding the training vector that minimizes the Euclidean distance for every validation image. We repeat this exercise for the first 10 images in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Your code goes here --\n",
    "for val_id, v in zip(val_ids[:10], validation_vectors[:10]):\n",
    "    # initialize defaults\n",
    "    best_dist = 99999999\n",
    "    best_vec = None\n",
    "    best_id = None\n",
    "\n",
    "    # find closet vector, store info\n",
    "    print(\"\\n\")\n",
    "    for train_id, t in zip(train_ids, training_vectors):\n",
    "        dist = np.linalg.norm(v-t)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_vect = t\n",
    "            best_id = train_id\n",
    "\n",
    "    # print closest images\n",
    "    val_img_str = val_id_to_file[val_id]\n",
    "    train_img_str = train_id_to_file[best_id]\n",
    "\n",
    "    # show images\n",
    "    pil_im = Image.open(val_img_str, 'r')\n",
    "    pil_im.show()\n",
    "    pil_im = Image.open(train_img_str, 'r')\n",
    "    pil_im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training a Multi-Label Classification Network\n",
    "\n",
    "We now build a two layer classification network, which takes 4096-dimensional vectors as input and outputs the probabilities of the 80 categories present in MSCOCO. \n",
    "\n",
    "For this purpose, we utilize two layers (both containing sigmoid activation functions) with the hidden dimension set to 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we construct a class for the model\n",
    "# -- Your code goes here --\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_in, dim_hidden)\n",
    "        self.linear2 = nn.Linear(dim_hidden, dim_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-5d5444da2ddc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Finally train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-5d5444da2ddc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# The output data is prepared by representing each output as a binary vector of categories\n",
    "# -- Your code goes here --\n",
    "\n",
    "def train(model, learning_rate=0.001, batch_size=100, epochs=5):\n",
    "    \"\"\"\n",
    "    Training function which takes as input a model, a learning rate and a batch size.\n",
    "  \n",
    "    After completing a full pass over the data, the function exists, and the input model will be trained.\n",
    "    \"\"\"\n",
    "    # -- Your code goes here --\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    num_vectors = len(training_vectors)\n",
    "    batches = num_vectors // batch_size\n",
    "    \n",
    "    for _ in range(epochs):    \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i in range(batches):\n",
    "            batch = training_vectors[100*i:100*i+99]\n",
    "            torch_batch = torch.from_numpy(batch).float().cuda()\n",
    "            print(torch_batch.is_cuda)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(torch_batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "# Finally train the model\n",
    "model = Net(4096, 512, 80).cuda()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 18, 21: 19, 22: 20, 23: 21, 24: 22, 25: 23, 27: 24, 28: 25, 31: 26, 32: 27, 33: 28, 34: 29, 35: 30, 36: 31, 37: 32, 38: 33, 39: 34, 40: 35, 41: 36, 42: 37, 43: 38, 44: 39, 46: 40, 47: 41, 48: 42, 49: 43, 50: 44, 51: 45, 52: 46, 53: 47, 54: 48, 55: 49, 56: 50, 57: 51, 58: 52, 59: 53, 60: 54, 61: 55, 62: 56, 63: 57, 64: 58, 65: 59, 67: 60, 70: 61, 72: 62, 73: 63, 74: 64, 75: 65, 76: 66, 77: 67, 78: 68, 79: 69, 80: 70, 81: 71, 82: 72, 84: 73, 85: 74, 86: 75, 87: 76, 88: 77, 89: 78, 90: 79}\n"
     ]
    }
   ],
   "source": [
    "print(category_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. End-to-End Model Fine-tuning\n",
    "\n",
    "Instead of training *only* the final two layers, we now create an end-to-end model and train the entire thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we construct a class for the model\n",
    "# -- Your code goes here --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output data is prepared by representing each output as a binary vector of categories\n",
    "# -- Your code goes here --\n",
    "\n",
    "def train(model, learning_rate=0.001, batch_size=50, epochs=2):\n",
    "    \"\"\"\n",
    "    Training function which takes as input a model, a learning rate and a batch size.\n",
    "  \n",
    "    After completing a full pass over the data, the function exists, and the input model will be trained.\n",
    "    \"\"\"\n",
    "    # -- Your code goes here --\n",
    "    \n",
    "# Finally train the model\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now repeat step two using the end-to-end classifier.\n",
    "# -- Your code goes here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hyper-parameter Tuning\n",
    "\n",
    "Now we do a grid search over the learning rate and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Your code goes here --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
